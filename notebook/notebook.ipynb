{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA Microarray Machine Learning\n",
    "\n",
    "- Data source is from UCI Machine Learning Repo [https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq]\n",
    "- The dataset is a collection gene expressions of patients having different types of tumor: BRCA(breast), KIRC(kidney), COAD(colon), LUAD(lung) and PRAD(prostate).\n",
    "\n",
    "\n",
    "## Import Data and Exploration\n",
    "The datast contains 801 instances and 20531 features. It is a high dimensional low sample size dataset with multiple classes. First step of the project is importing the dataset and perform some explorations of the dataset to gain some basic understanding. In terms of visualizations, it is difficult to visualize because of high dimensions. t -Distributed Stochastic Neighbor Embedding or t-SNE is a technique that I used to perform dimension reduction to visualize the data in 2D.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0  gene_0    gene_1    gene_2    gene_3     gene_4  gene_5  \\\n",
      "0   sample_0     0.0  2.017209  3.265527  5.478487  10.431999     0.0   \n",
      "1   sample_1     0.0  0.592732  1.588421  7.586157   9.623011     0.0   \n",
      "2   sample_2     0.0  3.511759  4.327199  6.881787   9.870730     0.0   \n",
      "3   sample_3     0.0  3.663618  4.507649  6.659068  10.196184     0.0   \n",
      "4   sample_4     0.0  2.655741  2.821547  6.539454   9.738265     0.0   \n",
      "\n",
      "     gene_6    gene_7  gene_8     ...      gene_20521  gene_20522  gene_20523  \\\n",
      "0  7.175175  0.591871     0.0     ...        4.926711    8.210257    9.723516   \n",
      "1  6.816049  0.000000     0.0     ...        4.593372    7.323865    9.740931   \n",
      "2  6.972130  0.452595     0.0     ...        5.125213    8.127123   10.908640   \n",
      "3  7.843375  0.434882     0.0     ...        6.076566    8.792959   10.141520   \n",
      "4  6.566967  0.360982     0.0     ...        5.996032    8.891425   10.373790   \n",
      "\n",
      "   gene_20524  gene_20525  gene_20526  gene_20527  gene_20528  gene_20529  \\\n",
      "0    7.220030    9.119813   12.003135    9.650743    8.921326    5.286759   \n",
      "1    6.256586    8.381612   12.674552   10.517059    9.397854    2.094168   \n",
      "2    5.401607    9.911597    9.045255    9.788359   10.090470    1.683023   \n",
      "3    8.942805    9.601208   11.392682    9.694814    9.684365    3.292001   \n",
      "4    7.181162    9.846910   11.922439    9.217749    9.461191    5.110372   \n",
      "\n",
      "   gene_20530  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "2         0.0  \n",
      "3         0.0  \n",
      "4         0.0  \n",
      "\n",
      "[5 rows x 20532 columns]\n",
      "  Unnamed: 0 Class\n",
      "0   sample_0  PRAD\n",
      "1   sample_1  LUAD\n",
      "2   sample_2  PRAD\n",
      "3   sample_3  PRAD\n",
      "4   sample_4  BRCA\n",
      "\n",
      "\n",
      "The shape of dataset:\n",
      "(801, 20531)\n",
      "(801, 2)\n",
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 801 samples in 0.553s...\n",
      "[t-SNE] Computed neighbors for 801 samples in 20.417s...\n",
      "[t-SNE] Computed conditional probabilities for sample 801 / 801\n",
      "[t-SNE] Mean sigma: 48.192174\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.251804\n",
      "[t-SNE] Error after 300 iterations: 0.820377\n",
      "TSNE took at 28.38 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "c of shape (801, 1) not acceptable as a color sequence for x with size 801, y with size 801",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('Class', None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[1;32m   4273\u001b[0m                 \u001b[0;31m# must be acceptable as PathCollection facecolors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4274\u001b[0;31m                 \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4275\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid RGBA argument: {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;31m# tuple color.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid RGBA argument: 'Class'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2215686e4a1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mx_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0my_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3468\u001b[0m                          \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3469\u001b[0m                          \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3470\u001b[0;31m                          edgecolors=edgecolors, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   3471\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3472\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[1;32m   4277\u001b[0m                 raise ValueError(\"c of shape {} not acceptable as a color \"\n\u001b[1;32m   4278\u001b[0m                                  \u001b[0;34m\"sequence for x with size {}, y with size {}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4279\u001b[0;31m                                  .format(c.shape, x.size, y.size))\n\u001b[0m\u001b[1;32m   4280\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4281\u001b[0m             \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# use cmap, norm after collection is created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: c of shape (801, 1) not acceptable as a color sequence for x with size 801, y with size 801"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# load the dataset\n",
    "X = pd.read_csv(\"../data/RNA_data/data.csv\")\n",
    "Y = pd.read_csv(\"../data/RNA_data/labels.csv\")\n",
    "\n",
    "# basic overview of data dimension\n",
    "print(X.head())\n",
    "print(Y.head())\n",
    "\n",
    "# convert dataframe into a numpy array\n",
    "X = X.dropna()\n",
    "# drop the first column which only contains strings\n",
    "X = X.drop(X.columns[X.columns.str.contains('unnamed', case=False)], axis=1)\n",
    "print(\"\\n\\nThe shape of dataset:\")\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# label encode the multiple class string into integer values\n",
    "Y = Y.drop(Y.columns[0], axis=1)\n",
    "Y = Y.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# use TSNE to visualize the high dimension data in 2D\n",
    "t0 = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state=100)\n",
    "tsne_results = tsne.fit_transform(X)\n",
    "t1 = time.time()\n",
    "print(\"TSNE took at %.2f seconds\" % (t1 - t0))\n",
    "\n",
    "# visualize TSNE\n",
    "x_axis = tsne_results[:,0]\n",
    "y_axis = tsne_results[:,1]\n",
    "plt.scatter(x_axis, y_axis, c=Y, cmap=plt.cm.get_cmap(\"jet\", 100))\n",
    "plt.colorbar(ticks=range(10))\n",
    "plt.clim(-0.5, 9.5)\n",
    "plt.title(\"TSNE Visualization\")\n",
    "plt.show\n",
    "plt.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The TSNE algorithm is a dimension reduction algorithm that allows us to visualize high dimensional data in 2D. We see that dataset form five clusters, which correlates with the five different tumor types.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Train and Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing set\n",
    "X_train, X_test, Y_train, Y_test \\\n",
    "    = train_test_split(X, Y, test_size=0.40, random_state=100)\n",
    "\n",
    "# save the train and test csv files\n",
    "X_train.to_csv(\"data/X_train.csv\")\n",
    "X_test.to_csv(\"data/X_test.csv\")\n",
    "Y_train.to_csv(\"data/Y_train.csv\")\n",
    "Y_test.to_csv(\"data/Y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In order for dataset to feed into machine learning algorithms, every feature and label intended for machine learning should be numerical and scaled appropriately. In previous step, we have transformed the labels into numerical value in preparing for t-SNE. The RNA expression features are already expressed in float numbers. Scaling is not neccessary here because all feature data came from microarray. The only operation we need to perform is transforming panda dataframe into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "X_train = pd.read_csv(\"data/X_train.csv\").values\n",
    "Y_train = pd.read_csv(\"data/Y_train.csv\").values\n",
    "\n",
    "X_test = pd.read_csv(\"data/X_test.csv\").values\n",
    "Y_test = pd.read_csv(\"data/Y_test.csv\").values\n",
    "\n",
    "# transform panda df into arrays\n",
    "X_train = np.delete(X_train, 0, axis=1)\n",
    "Y_train = np.delete(Y_train, 0, axis=1).flatten()\n",
    "\n",
    "X_test = np.delete(X_test, 0, axis=1)\n",
    "Y_test = np.delete(Y_test, 0, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "\n",
    "Considering the high dimensionalities nature of the problem, a basic linear classifier would have a hard time to come up with a clear boundary for classification. I chose four models that may be effective to work with high dimensional data. Then, I run grid search to find the optimal parameters for each model.\n",
    "\n",
    "###  Linear Support Vector Machine (SGD)\n",
    "\n",
    "It implements a linear support vector machine model using `hinge` loss function. It allows for minibatch learning, which means the model will continue to learn based on its learning rate, until the maximum iterations is reached or model stop improving\n",
    "- penalty: default is `l2`, which is not suitable for high dimensional data. `elasticnet` and `l1` may overcome this problem by introducing sparsity in the feature space\n",
    "- alpha: constant that multipilies the regularization term. Also determines learning rate when learning rate is set to optimal. Values tested are 0.0001, 0.5, 1, 5, 50, 100, 200, and 500\n",
    "- learning rate: can be constant, optimal or invscaling. \n",
    "\n",
    "### Non-linear Support Vector Machine (SVC)\n",
    "\n",
    "Grid searched for three different non-linear kernel including `rbf`, `sigmoid`, and `poly`. Other parameters tuned including\n",
    "- C: error term. Values test are 1, 10, 100, 1000\n",
    "- gamma: kernel coefficient. Values tested are 0.001, 0.0001, and 0.00001\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "Random Forest is an algorithm that excels at finding complex hidden patterns in the data. It uses a collect of decision trees to fit, then use the average to improve predictive accuracy. The two parameters I set are n estimator and max leaf nodes. \n",
    "- n estimator: the maximum number of tree. Values tested are 10, 20, and 50\n",
    "- max leaf nodes: the maximum of leaf nodes. Values tested are 50, 100, 150 ,and 200\n",
    "\n",
    "### Neural Network MLP\n",
    "\n",
    "A multi-layer perceptron algorithm is used. It trains using backpropagation and supports multi-class classification by applying softmax as output function. It has the most options for parameter tuning\n",
    "- hidden layer size: the number of hidden layers. values tested are 50, 100, 150, and 200\n",
    "- alpha: L2 penalty (regularization term). Values tested are 0.0001, 0.0005, 0.001, and 0.005\n",
    "- activation: activation function for hidden layer. Options tested are `relu`, `tanh`, and `identity`\n",
    "- solver: the solver for weight optimization. `lbfgs` is optimal for smaller sample size. `Adam` is more suited for larger sample size. `sgd` refers to stochastic gradient descent\n",
    "- learning rate: sets the update schedule, `adaptive` keeps initial rate as long as training loss keep decreasing and decrease the rate when training loss goes up. Other two learning rate tested are `constant` and `invscaling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import svm, ensemble, linear_model\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the models\n",
    "sgd_clf = linear_model.SGDClassifier(random_state=100, n_jobs=-1)\n",
    "svm_clf = svm.SVC(random_state=100)\n",
    "rf_clf = ensemble.RandomForestClassifier(random_state=100, n_jobs=-1)\n",
    "nn_clf = MLPClassifier(random_state=100)\n",
    "\n",
    "# paremeter tuning for sgd, by default sgd fits a linear svm\n",
    "parameters = {\n",
    "    'alpha': [0.0001, 0.5, 1, 5, 50, 100, 200, 500],\n",
    "    'penalty': ('l2', 'l1', 'elasticnet'),\n",
    "}\n",
    "sgd_clf = GridSearchCV(estimator=sgd_clf, param_grid=parameters).fit(X_train, Y_train)\n",
    "print(\"Best params for sgd:\", sgd_clf.best_params_, '\\n')\n",
    "\n",
    "# parameter tuning for non-linear svm kernel\n",
    "parameters = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'gamma': [0.001, 0.0001, 0.00001],\n",
    "    'kernel': ('poly', 'rbf', 'sigmoid')\n",
    "}\n",
    "svm_clf = GridSearchCV(estimator=svm_clf, param_grid=parameters).fit(X_train, Y_train)\n",
    "print(\"Best params for svm:\", svm_clf.best_params_, '\\n')\n",
    "\n",
    "# parameter tuning for random forest\n",
    "parameters = {\n",
    "    'n_estimators': [10, 20, 50],\n",
    "    'max_leaf_nodes': [50, 100, 150, 200]\n",
    "}\n",
    "rf_clf = GridSearchCV(estimator=rf_clf, param_grid=parameters).fit(X_train, Y_train)\n",
    "print(\"Best params for rf:\", rf_clf.best_params_, '\\n')\n",
    "\n",
    "# parameter tuning for neural network\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [50, 100, 150, 200],\n",
    "    'alpha': [0.0001, 0.0005, 0.001, 0.005],\n",
    "    'activation': ('relu', 'tanh', 'identity'),\n",
    "    'solver': ('lbfgs', 'sgd', 'adam')\n",
    "}\n",
    "nn_clf = GridSearchCV(estimator=nn_clf, param_grid=parameters).fit(X_train, Y_train)\n",
    "print(\"Best params for nn:\", rf_clf.best_params_, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation \n",
    "K-fold cross validation is used to evaluate the performance of each model without touching the test dataset. Number of folds is set to 5. The dataset is partitioned into 5 sets, with 1 set being the testing data and rest 4 being the training data until all sets have taken turns. An alternative approach would be using leave one out, but it is much more computational intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation to select the best model\n",
    "def kfold_model_score(model, X_train, Y_train, numFolds=5):\n",
    "    k_fold_shuttle = KFold(n_splits=numFolds, random_state=100).get_n_splits(X_train, Y_train)\n",
    "    return np.mean(cross_val_score(model, X_train, Y_train, cv=k_fold_shuttle))\n",
    "\n",
    "\n",
    "sgd_clf_score = kfold_model_score(sgd_clf, X_train, Y_train)\n",
    "print(\"Linear SVM score: {:5f}\\n\".format(sgd_clf_score.mean()))\n",
    "\n",
    "svm_score = kfold_model_score(svm_clf, X_train, Y_train)\n",
    "print(\"Non-linear SVM score: {:5f}\\n\".format(svm_score.mean()))\n",
    "\n",
    "rf_score = kfold_model_score(rf_clf, X_train, Y_train)\n",
    "print(\"Random Forest score: {:5f}\\n\".format(rf_score.mean()))\n",
    "\n",
    "nn_score = kfold_model_score(nn_clf, X_train, Y_train)\n",
    "print(\"MPL Neural Network score: {:5f}\\n\\n\".format(nn_score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
